

# Internet Protocol suite


[toc]

---

[wiki of internet protocol suite](https://en.wikipedia.org/wiki/Internet_protocol_suite)

<img src="learn-internet.assets/OSI模型.png" alt="OSI模型" style="zoom:50%;" />

<img src="learn-internet.assets/v2-ec64022ca37ce5b1de17d98a3d3bbf86_1440w.jpg" alt="img" style="zoom:50%;" />

### 一些小问题

* 网络不管是那个部分，都会分客户端和服务器，初学者可能总是认为客户端指的是我们自己的电脑，而服务器指的是那种大机房。很多情况下是的，但很多时候并不是，只要是等待数据报并作出回应的基本都可以看作服务器，而服务器更多的是面向进程的一个概念，也就是一台设备可以同时有客户端进程和服务器进程。
* 初学者，尤其是计算机大方向的初学者总是不自觉的将计算机代入成人，这是非常错误的想法，想其原因，我猜测可能是现在的（2020后）计算机的确越来越“智能”，但是它们目前仍是机器，100%的机器，十分精密且复杂到可以自动化的实现人类预先设计好的让他们工作的步骤。这些人类是好多代聪明的科学家和工程师的思想，而没有一个是“计算机”想出来的，所以比如我为什么连上WiFi就可以上网了？这背后的机制非常复杂，但都是人类想法的实现。





## 物理层

### 信息传输的基本概念





### 1.传输速度/时延、传播速度/时延 的关系

1. 比如带宽100Mb/s，到底是什么意思？带宽和传输速度有什么关系？
2. 为什么有人把传输速度比喻成「车道宽度」或「漏斗管子横截面积」？这合理吗（非常不合理）
3. 这些bits是串行的吗？一根网线的极限传输速度是由于什么被限制？物理极限大约为多少？
4. 哪些“人为”因素限制了用户的传输速度；延迟是什么？受限于什么因素？和传输速度有关吗？
5. 同时包括延迟和传输速率限制的完整的传输过程是什么样子的？



虽然光纤的理论传输速率很高(TB量级)，但是由于现在的网络结构，一根光纤往往同时承载大量网络节点的数据传输，所以平均每台设备的分得的传输速率就少很多。加下图：

<img src="learn-internet.assets/Screen Shot 2021-10-07 at 01.10.44.png" alt="Screen Shot 2021-10-07 at 01.10.44" style="zoom: 25%;" />

#### 延迟与传播速度关系不大



##### 内容提供商限制传播速率

> 快递的例子，假设我贼有钱，在京东自营上买了10万件东西，一件一单。这时候京东收到了我的请求，开始去仓库找货(10万件假设1秒找一件)，然后包装，写上我的收货地址，发货，中间有经过了n多次中转，终于2天后收到了第一个包裹，而在接下来的时间里，不出意外就是1秒收到一件。
>
> **「2天」就是延迟，而1秒1件就是网速。**

若不间断的给我发件，我收件的速度一直会是1s1件，这时便感觉不到延迟的存在，这时的速度，才被称为传播速度。

而事实上，往往是相反的情况：假设用任意门发快递，但是京东人手不够，发货速度最快1h一件，但是由于任意门，只要发货，几乎瞬间就能到。这时延迟就极低，但是传输速度却低了很多，变成了1h1件。

以上讲到的传输速率都是取决于京东发货的速度，这对应到现实就是：没有买会员，服务器给你转发的慢。



##### 运营商限制传播速率

若你买了10M宽带，这就对应着，任意门1h只给你一件货，并且会通知京东，我这里1h1件货，你不用1s1件的发货了(这就是TCP的流量控制)。



##### 物理层限制传播速率

若你很有钱，开了1000M的宽带，并开了腾讯超级会员。但是你的网线是100M的，这意味着什么？这是因为，任意门的延迟虽然很低，但是门很小，每次(搬一次10ms)只能搬一个小包，大包还需要分开搬。



所以任意门很快，几乎瞬间到，但是如果持续发送包裹，还是收到任意门大小的限制**（这个限制在网线上就是香浓定理和编码规则）**。



#### 平均传播速率的计算



##### (1)分组存储转发的情况

![IMG_1937](learn-internet.assets/IMG_1937.jpeg)

条件是按照2010年这个真题设定，首先假设：

1. 这是一个永远以传输速率（此题是100Mb/s）进出“站点”（即链路端口）的队伍，一个人就是一个bit。

2. 数据大小就是队伍的长度，分组大小就是小队的长度，比如此题就是：1M人组成的队伍，分成1k个小队，每个小队1k人。

3. 若传播速率按照题目设定是∞，则指队伍排队出来后需要0s就可以通过任意门（网线）到达下一个“站点”。当然进站速度也是受限于此“站点”的极限传输速率。

4. 每个中间“站点”（路由器），都有一个“小队大小的缓冲区”。人进来之后会在这里排队，等待小队人到齐后（缓冲区满），依次排队出站（依然是受限于此“站点”的极限传输速率）。



##### (2)GBN延迟“拖累”窗口发送的情况

见下文---《TCP---GBN与SR》



### 2. 电磁波的“穿透力”？

所有波，应该都有一个共性：低频波衍射能力强，高频波穿透能力强。(But why？)



* 下面[引文](https://www.wpgdadatong.com/cn/blog/detail?BID=B1196)，也没有解释为什么，而是用“实验”显示了，衍射现象在波长长的时候更容易发生。

> 只有缝、孔的宽度或障碍物的尺寸跟波长相差不多或者比波长更小时，才能观察到明显的衍射现象。但当孔的尺寸远小于波长时（小于波长的3/10时），尽管衍射十分突出，由于能量减弱，衍射现象已不容易观察。
>
> ![img](https://edit.wpgdadawant.com/uploads/news_file/blog/2020/1465/tinymce/______2.jpg)



* 下面选自[一知乎回答](https://www.zhihu.com/question/59506734/answer/166405974):

> 光衍射的本质是光绕过较小的障碍物，与直线传播不符的一种现象。那这么想一下，如果障碍物太大，光根本绕不过去呀，所以要求障碍物小于光的波长。
>
> 我们可以看出当宽度大于波长的时候，根本不会在缝附近产生次波的相干叠加，也就没有衍射现象。



还是没有令人信服的理论，先不管为什么了，假设是正确的。所以X射线可以用来骨成像，而5G频率的WiFi“穿墙能力”比2.4G频率的差。



### 3. 电磁波的频率

由于编码方式主要针对于一周期的周期信号，所以频率高的信号，自然承载的信号量就多，网线中交流电的100MHz量级，远远小于电磁波的GHz-THz量级。

<img src="learn-internet.assets/Electromagnetic-Spectrum.svg" alt="Electromagnetic-Spectrum" style="zoom:25%;" />

### 4. 频宽与极限传输速率---傅立叶变换

下文中我讲到<>---<>

> 频分要真正的理解需要理解傅立叶变换。总之就是将一段大的频率范围，划分为几段小的，分给不同的设备，这样同时发送信号，也能区别出来自己所被分配频率的信号。但是这也是每个设备牺牲了其余频率的信息(频宽也意味着带宽)换来的“时间”。所以天下没有免费的午餐，哈哈哈。
>
> > 也就是傅立叶级数------一个“任意”的随时间变化函数(也就是信号)，可以写成N(很大)个频率为nπ(n是从0到N的正整数)的正弦信号，每个信号的幅值不同。这样比如我们可以把N个幅值作为作为我们要传递的信息。所以接收者只需要把看起来没有规律的时间信号作傅立叶变换，就可以得到不同频率的幅值。
> >
> > 进一步抽象来看，不同频率的正弦波其实就是一组基向量，而发送的随时间变化的信号(离散的)，就是在这组基向量展开的空间中的一个向量。而每个频率的幅值，正是该空间沿这组基向量的投影。（具体见《信号分析》的教材）
> >
> > 若我们规定每个设备的频率范围不同，则接收者只需要将信号傅立叶变换后，拿到这些频率对应的幅值即可。(其余频率对应的幅值是别的设备的数据)

当你被分配1Hz、2Hz、...1000Hz的这些频率的信号时，就约等于你获得了1000Hz的频宽（有时也叫带宽，但注意和传输速率的带宽区分）。你若规定，每个频率的周期信号幅值只有两种0，和k(k可以为10)，这就代表0和1，也就是一个码元对应两个bit。

![IMG_2131](learn-internet.assets/IMG_2131.jpeg)

这时假设你想发送0101010101010101.....0101，则就把这1000个频率的sin函数分别乘以这1000个0或1，然后相加得到了一个看起来很负责的函数曲线，把他作为信号发出去，接收者拿着这个信号作傅立叶变换就得到了这些0101010101....0101。

> 具体来讲，信号是利用的离散傅立叶变换，为什么函数可以看作基向量呢？也是从离散说起：
>
> 
>
> 准确的来说，这是离散傅里叶变换。其中蕴含着一个非常深刻的道理：**函数即向量。**若我们把一个函数每个1单位的自变量范围等分n（假设n=1000）份，则一个周期2π内，2π×1000个函数值就可以看作一个2π×1000维的向量:[$$sin(\frac{1}{1000}),sin(\frac{2}{1000}),...,sin(\frac{1001}{1000},...,sin(2\pi))$$]。这样划分有什么意义呢？
>
> 我们由积分可以知道，$$\int_{1}^{nT} sinx×sin2x = 0$$(在n个周期内积分都是0)。若我们把sinx和sin2x都写成这样的向量，那么积分本质上就是累加，这个积分也就是这两个向量做内积:$$\sum\limits_{i=1}^{nT} sin(\frac{i}{n})×sin(\frac{2i}{n})=0$$。它们内积为0，这就是正交！
>
> 同理，sinx和sin3x、sin4x等相乘积分都为0，也就是这n个函数划分的n个1000维向量（假设βi就是这n个向量）是相互正交的！这又有什么意义呢？
>
> 傅立叶来了，“任意”函数【假设是f(x)】可以写成$$\sum\limits_{j=1}^{W}Aj×sin(jx)$$。函数离散化为向量，每个sin(jx)也是离散化的相互正交的向量(也就是基向量)，所以Aj们就是f(x)这个向量在每个基向量的投影。^_^





### 物理层架构



巴拉巴拉

![物理层](learn-internet.assets/物理层.png)

### 奈氏准则



> [知乎回答](https://www.zhihu.com/question/40443733/answer/1459206942)
>
> 







## 链路层Link layer

巴拉巴拉

![链路层](learn-internet.assets/链路层.png)

### MAC

在链接层，计算机把自己本地的数据按照一定的格式封装实现以太网(Ethernet)之间的传输。也就是所谓的MAC封装(Media Access Control)，标准为：(**CSMA/CD**)。

MAC地址是唯一的设备号，但是只在局域网中生效，也就是link layer这一层。因为当你和internet交流时，需要一个“路由”，此时相当于它和外网服务器组成局域网，就是它的MAC地址，自己计算机的MAC就被“隐藏了”。

**exponential backoff**：

事实上Mac地址很像我们的身份证号，从出场就写入硬件了（网卡）。但是就像有人违规用假身份证号类似，Mac地址也可以“骗过”操作系统，“更改/伪造”Mac地址。

#### MAC帧格式

![MAC帧格式](learn-internet.assets/MAC帧格式.jpeg)

#### MAC帧的最小长度与碰撞窗口

> A在发送帧后至多经过时间2(端到端传播时延的2倍)就能知道所发送的帧有没有发生碰撞。因此把以太网端到端往返时间2x称为争用期(又称冲突窗口或碰撞窗ロ)。
>
> 例如,以太网规定取512s为争用期的长度。对于10Ms的以太网,在争用期内可发送512bi, 即64B。在以太网发送数据时,如果前64B未发生冲突,那么后续数据也就不会发生冲突(表示已成功抢占信道)。换句话说,如果发生冲突,那么就一定在前64B。由于一旦检测到冲突就立即停止发送,因此这时发送出去的数据一定小于64B。因此,以太网规定最短帧长为64B,凡长度小于64B的帧都是由于冲突而异常中止的无效帧,收到这种无效帧时应立即丢弃。

其中有很多问题：

比如CSMA/CD又不是那种，发之前先看一看别人有没有发。所以接下来发，还是会继续碰撞检测啊…怎能抢占信道呢？



### 链路层的可靠传输

> 节选自黑书：
>
> 链路层可靠交付服务通常用于易于产生高差错率的链路，例如无线链路，其目的是本地(也就是在差错发生的链路上）纠正一个差错，而不是通过运输层或应用层协议迫使进行端到端的数据重传。然而，对于低比特差错的链路，包括光纤、同轴电缆和许多双绞铜线链路，链路层可靠交付可能会被认为是一种不必要的开销。由于这个原因，许多有线的链路层协议不提供可靠交付服务。



### 多路访问冲突问题和协议

对于网络上N个设备之间的相互信息传输，我们当然希望它是可以同时发生的，但是恐怕只有建立$$C_n^2$$条专线才能大致实现。



#### 现代网络结构

现实的网络是一个中心辐射型的网状结构，但这并不是完全的集中，而是：

* 「骨干网」，可以看作整个现代网络的中心。也就是第一层大型网络转发处理设备(服务器)相互连接，它们是相对平衡对称的，它们没有一个中心的“网关”来进行协调分配转发等工作的，而是每个节点平等，这也不是必然，而是目前现代社会网络结构选择了这种实现方式(当然要说道理肯定是有的，比如中心中网络太依赖核心节点/网关的稳定性及超大带宽)。

  另外，网络结构很大一部分取决于路由选择的方式，也就是对称型的，被称为“每路由控制”；还是有统一服从集中调配型(SDN)，被称为“逻辑集中型控制”。

* 「用户端网」，从地区ISP往“下”，可以看作整个现代网络的这个网络就变成了完全集中式，像我们的家用设备都会链接到一个路由器上，路由器作集中转发。

> 上面这段需要结合下文中网络层的路由选择问题来看。
>
> <img src="learn-internet.assets/Screen Shot 2021-10-07 at 11.22.07.png" alt="Screen Shot 2021-10-07 at 11.22.07" style="zoom:20%;" />
>
> 比如我访问微信的官网，可以看到跳转经过这些路由或服务器，最终被转发到微信官网的服务器，我大至画了网络结构图，当然肯定有不准确的地方：
>
> <img src="learn-internet.assets/IMG_1843.jpeg" alt="IMG_1843" style="zoom: 25%;" />
>
> 从我的电脑，家用路由器，地区ISP，这三级其实是一个树状的网络，子节点若给“外网”发送数据，则无脑转发父节点（路由器）即可。但是从地区ISP开始，他们与骨干网组成了一个复杂的，“去中心化”的网络结构，它们之间的转发不再有“父节点”。
>
> 当然骨干网和狭义的广域网差不多一个意思，且，其实核心网络是由许多大型的交换机连接起来，这并不是普遍意义的交换机，具体见下部分内容的⑤问题：



#### 关于以太网的几个重要问题

就是链路层及物理层的网络协议。遵从这个协议（CSMA/CD）规范连接的网络就是以太网。至于为何这么多网络协议，这么多层次的比如IP层的众多协议，为何只有这个被冠之以“以太网”这种听起来很牛逼的名字？没有为什么，就是一群人抽抽了估计。



**①以太网与局域/广域网？**

局域网、广域网不是一个严谨的技术概念，所以完全不是一回事。但是广域网一般来说，代指现代网络结构中的核心网络，其是一个去中心化的大网。



**②以太网和链路层IP层？**

**以太网是一个协议（CSMA/CD）**，它是链路层的协议，所以一个网络可以在链路层不用以太网协议，但是同样有IP层、遵守TCP协议等。

另外，网络层次也没有太严格的区分，比如IP“层”，其实就是围绕IP地址、路由转发等一系列协议，一个路由器，可能只是按需要实现其中的几个协议，不需要全部按部就班的实现(当然最核心的如IP数据报格式，肯定是严格遵守并实现的)



**③以太网和无线网？**

无线网也不是个严谨技术概念，准确的说是：WI-FI、蓝牙、NFC、蜂窝移动网络、Zigbee等。

以太网的确跟他们不是一回事，且它们基本属于同一层次的概念，比如WiFi就是CSMA/CA协议。是因为传输媒介都变了，接口标准，网络连接面临的挑战也不一样，协议内容必然不同。

> 本段引用自《计算机网络 自顶向下方法》
>
> 我们也把沿着通信路径连接相邻节点的通信信道称为链路(link）。
> 为了将一个数据报从源主机传输到目的主机，数据报必须通过沿端到端路径上的各段链路传输。举例来说，显示在图6-1 下部的公司网络中，考虑从无线主机之一向服务器之一发送一个数据报。该数据报将实际通过6 段链路：发送主机与 wiFi 接人点之间的 wiFi 链路，接人点和链路层交换机之间的以太网链路，链路层交换机与路由器之间的链路，两台路由器之间的链路，最后是交换机和服务器之间的以太网链路。在通过特定的链路时，传输节点将数据报封装在链路层帧中，并将该帧传送到链路中。

**④光纤与双绞线都可以组“以太网”？**

应该不可以。虽然以太网主要定位是一个链路层的协议，但是以太网同时定义了有线网络接口(RJ45网线口)，所以它是一个包含物理层与链路层的协议。(当然中间只需要一个物理信号转换的设备就可以了，这个设备不需要运行复杂的网络协议)



**⑤比如骨干网ISP组成的网络，它们之间并没有一个“网关”来负责转发等工作，这种网络是以太网吗？若不是，是什么，链路层有何区别？**

**骨干网等大型路由组成的去中心化网络结构，属于点对点通信(它们一般使用PPP或HDLC协议来在链路上传输MAC帧)**，他们没有一个“网关”，这就不属于以太网协议，主要是他们通信时的问题不一样，但点对点也是遵从MAC协议的，所以他们之间是可以交流数据包的。

<img src="learn-internet.assets/IMG_2056.jpeg" alt="IMG_2056" style="zoom:25%;" />

上图中，网络是由一些大型节点交换机来负责转发的 (具体交换机如何转发，见下节《交换机的转发》)，但这个交换机，不要将其理解为狭义的链路层的交换机。

其实**一台计算机**(无论是服务器还是用户主机还是家用路由器)**是运行在链路层，还是在IP层，取决于他在运行什么样的进程。而现实中尤其是大型交换机，它们其实同时负责路由的。（也就是二层交换机和三层交换机的区别）**，具体见[link](https://www.zhihu.com/question/20465477/answer/657327623)。正如协议也不用全部实现，这其实是很灵活的，达到最终的目的就可以。



---



多路传输问题，有时被称为介质访问控制，整的名字挺扯，其实就是一车道的桥，桥这头儿n个目的地，那头n个目的地，如何双向如何通行的问题。

* 静态也就是划分信道，把一条单行宽车道改成多条单行宽车道，最好一人一条路，自己走自己的路。

* 动态就是动态的“管控”自己的行为(是否要发，何时发)，大家一条路，比如上桥之前看看有没有车再桥上。



**划分信道，可以分为三种解决方式：**

* 时分多址
* 频分多址
* 码分多址

时分最容易理解，比如把1s分成10分，100ms一份，每100ms中只有一个设备能发送数据。

频分要真正的理解需要理解傅立叶变换。总之就是将一段大的频率范围，划分为几段小的，分给不同的设备，这样同时发送信号，也能区别出来自己所被分配频率的信号。但是这也是每个设备牺牲了其余频率的信息(频宽也意味着带宽)换来的“时间”。所以天下没有免费的午餐，哈哈哈。

> 也就是傅立叶级数------一个“任意”的随时间变化函数(也就是信号)，可以写成N(很大)个频率为nπ(n是从0到N的正整数)的正弦信号，每个信号的幅值不同。这样比如我们可以把N个幅值作为作为我们要传递的信息。所以接收者只需要把看起来没有规律的时间信号作傅立叶变换，就可以得到不同频率的幅值。
>
> 进一步抽象来看，不同频率的正弦波其实就是一组基向量，而发送的随时间变化的信号(离散的)，就是在这组基向量展开的空间中的一个向量。而每个频率的幅值，正是该空间沿这组基向量的投影。（具体见《信号分析》的教材）
>
> 若我们规定每个设备的频率范围不同，则接收者只需要将信号傅立叶变换后，拿到这些频率对应的幅值即可。(其余频率对应的幅值是别的设备的数据)

码分多址多址相对更复杂一些，但是本质上和傅立叶变换有些类似。这是在用数据长度换时空(时间和频宽意味着带宽)。（划分N个就需要N倍原始数据量来传递数据，无效数据量的增加就是相当于减少了时空/带宽）

> 因为不同频率的正弦波，就是相互正交的“基”向量。而这个码分多址，就是把一段二进制数据本身看作基向量，人为把N个正交的N位二进制数据定义为不同设备的基向量。同样你的设备接收到了时间信号，若想“看”某个设备的信息，就把这个信号与对应设备的“基向量”做内积（也就是投影），就得到了该设备发送的信息。



**在现实的协议规范中，多种方式是结合使用的，甚至会动态的调整方案**。比如WiFi6中，即用到了时分、频分，还用到了接下来同信道的碰撞避免问题(CSMA/CA)。

正如这部分开头我举的例子，划分多车道(信道划分)，与上桥前下车看一看(碰撞检测避免)，现实中是划分了好几条车道，但是只要每条车道不是专用单行车道，那么还是需要再每条车道上进行“碰撞检测”。

> 想象一下，若我们规定，将200MHZ-1000MHZ分成100份；再将每个信道的每秒划分10份，即100ms一份；再用64个64bit的数据(“基向量”)来表示每组设备的“基数据”。
>
> 这样，理论上就可以有64×10×100=64000个不同的设备，可以完全不会冲突的发送与接收数据了。



**碰撞问题，有两种主要解决方式：**

一种是检测碰撞(CSMA/CD)，一种是避免碰撞(CSMA/CA)。



#### 1.碰撞与解决方案

针对于这种复杂情况，我们如何解决可能数据会同时收发，引起信号冲突的问题。当然这在物理层就提到过的一个不错的方法：信道划分。但这不是唯一的解决方案，这些不同的解决方案的关系是什么？会不会嵌套使用？接下来在讲，现在我们先看下：在理想情况下，对于速率为 R bps 的广播信道，我们希望多路访问协议应该具有以下的特性：

1. 当仅有一个节点发送数据时，该节点具有Rbps 的吞吐量；
2. 当有M个节点发送数据时，每个节点吞吐量为 R/M bps。这不必要求 州个节点中的每一个节点总是有R/M 的瞬间速率，而是每个节点在一些适当定义的时间间隔内应该有R/M的平均传输速率。
3. 协议是分散的；这就是说不会因某主节点故障而使整个系统崩溃。
4. 协议是简单的，使实现不昂贵。



这（检测碰撞并取消发送）会有个问题，就是大家都同时检测到要发，然后都取消，然后等待，就会饥饿。所以：

* 方案1:等待时间为随机数；
* 方案2:等待时间成指数增加(binary exponential backoff)。（范围是指数增加，还是在范围中选随机数）



#### 2.信道划分









### 交换机的转发



#### 每层协议的实现与否有所不同！

就像是链路层的可靠传输，这些都是网卡来实现的，而网卡是根据不同的应用场景，功能(某些协议的实现与否)是不一样的。比如上文提到无线传输的网卡会增加可靠传输，来避免到TCP之后再重传，而有线网卡一般就没有这个功能。

同样地，同样是转发，交换机的网卡实现了自学习构建其转发表，并进行转发，而对于我们的路由器、电脑，其网卡就不会又这个功能，路由器的转发是在网络层通过路由算法来构建的。

所以不代表，每一个设备，都实现每一层的每一个协议。这个是根据设备的定位灵活取舍的，只要整体的功能实现了就行。



**但是一定注意每层实现的区别在哪：**所谓传输层做的工作，也就是传到目的地才会干的事儿。

比如：

①TCP的GBN实现可靠传输，也就是如果中间路由器丢了这个数据报，这个路由器甚至都是不知道的，只有目的设备超时才会发送“你要重发”的消息。

②有些无线网络链路层传输时也会用GBN来保证数据传输的可靠性，这时这个数据报若是TCP的，那这就是套娃过程。

所以：

很多问题(协议)在所谓传输层考虑，关键在于，往往这些问题只需要这个数据报“到头儿”了才考虑的，而不是路由器的每一跳都要考虑，比如建立连接(IP要是建立连接，路由器每一跳都要先连接一遍，延迟太大了)



### ARP 协议/NDP协议

[ARP](https://zhuanlan.zhihu.com/p/28771785)

这是因为操作系统会有一个ARP缓存表，这就要说到这个ARP（IPv4）/DNP（IPv6中替代前者）协议。

> OSI模型把网络划分为七层，IP地址在OSI模型的第三层，MAC地址在第二层，彼此不直接交流。在通过以太网发送IP数据包时，需要先封装第三层（32位IP地址）、第二层（48位MAC地址）的报头，但由于发送时只知道目标IP地址，不知道其MAC地址，又不能跨第二、三层，所以需要使用地址解析协议ARP，根据网络层IP数据包包头中的IP地址信息解析出目标硬件地址（MAC地址）信息，以保证通信的顺利进行。



## Internet/Network layer

![网络层](learn-internet.assets/网络层.png)

在网络层，也就是IP的定义和封装了。

### Why IP？

Q：为什么都有MAC地址了，还需要IP地址，有了MAC地址设备对设备不就可以实现数据传输了吗？

A：这就是为什么有身份证号还要家庭住址，家庭住址就类似IP，表征了计算机在网络上的相对位置，而这个是可能会变化的，比如我搬家了。这也就是为什么需要MAC。IP的好处就像家庭住址的好处，寄快递的时候不用上来看你身份证号，而是看地址，一层层的走。而且也具有动态分配的诸多好处。



#### 问题：

**每一跳都要重写ip数据首部的源地址和Mac桢首部的源地址、目的地址吗？那数据传回时怎么还原？且重写源IP和网络地址转换（NAT）有哪些联系和区别？**

这是不是意味着MAC地址的重要性？因为目的IP地址不变，查询路由表确定下一条IP之后要由ARP缓存确定其MAC，才能发给下一跳（因为并没有指定目的IP是下一跳的目的IP）？





### Class Of IP


IP为了方便管理，分为了五级：

```
Class A :   0.xx.xx.xx - 127.xx.xx.xx
Class B : 128.xx.xx.xx - 191.xx.xx.xx
Class C : 192.xx.xx.xx - 223.xx.xx.xx
Class D : 224.xx.xx.xx - 239.xx.xx.xx
Class E : 240.xx.xx.xx - 255.xx.xx.xx
```

其中每级拿出一部分来作为局域网IP来使用，这些IP是没有办法接入互联网的，仅作为局域网IP使用，这样主要是为了解决设备多余IP地址的问题：

而且送你家里的快递，你爸妈直接叫你名字你就过来拿了（事实上有很多和你重名的人，但你在家是相对唯一可以确定的），这就是ip可以分级的好处，也称为**子网掩码**（Netmask）：255.255.255.0。实时的表示了现在你所在的网络位置，有时绝对，有时相对，方便找到你，就像是地址。但是这种只用于内部网络的IP并不能随便使用(类似于不能随便起名)，就是Private IP（家里给你起的名，或者小镇上某一个地点的别名），反之其他的成为Public IP（可以唯一确定的地址）。

**注意：下面几类私有IP地址跟子网掩码没有直接对应关系，这里容易产生误解，换句话说，公有IP地址也可以对应子网掩码。下面《关于分类编址、子网掩码、CIDR》会细讲。**

- Class A：10.0.0.0  - 10.255.255.255
- Class B：172.16.0.0 - 172.31.255.255
- Class C：192.168.0.0 - 192.168.255.255

由于这三段 Class 的 IP 是预留使用的，所以并不能直接作为 Internet 上面的连接之用，不然的话，到处就都有相同的 IP 啰！那怎么行！网络岂不混乱？所以啰，这三个 IP 网段就只做为内部私有网域的 IP 沟通之用。

**Q：**为什么私有IP要分级呢，有什么好处？

A：这是因为所谓局域网，家里的路由和计算机设备组成了局域网。那一个小区千百家的路由器其实也是在一个更大的局域网罢了，**（家外的一般属于互联网服务提供商ISP--Internet Service Provider）**并不是直接接入互联网的，原因也很简单，就像地址有国家，也有省，市，区一样，这个ClassA就像是省，仔细看ClassA给的可用IP地址也能知道，数量远远大于ClassB和C。这也就是为什么我们路由器的ip都是用如192.168.0.12来表示的，因为家里局域网不需要这么多设备，路由器也没有这么高的处理能力来给这么多IP解码分发数据。

ClassC其实可以再次切分。为了准确表示：**192.168.0.0/24（这是子网掩码）**。IPv4一共32位二进制，每个‘.’就是8位，像这个表示意思就是说：我只给你这个局域网分配了32-24

正如同地址，美国白宫的地址在可预见的时间内是不会变的，你家就几乎不可能搬到那里，所以ip也有静态ip，是某些大型机构的专属“地址”，这也让网络访问变得容易。





**Q：**有了IP，MAC地址在网络传输中还有什么作用？怎么核对它们？

A：虽然MAC只能点对点，在局域网内，也就是每个设备只存自己局域网的`ARP table`，里边包含了局域网内设备的MAC和IP的对应关系，然后路由器和“外网”也有一个大路由组建的局域网，以此类推。

**IP这一层也限制了每个数据包的大小**，同时还定义了路由协议，让包在网络节点上传播时更高效，灵活。



**Q：**我们的IP是虚拟IP，那么访问服务器时，必然只能使用公网那个“大”路由的公网IP地址，这必然会造成阻塞，怎么解决？

A：这就需要下面讲到的UDP/TCP协议，制定了端口，然后[NAT协议](https://zh.wikipedia.org/zh-cn/网络地址转换)用来把私有IP转换为公网IP的一个端口，相当于把我们电脑的请求变成路由器的一个“软件/进程”。所以，现代路由器，绝不是仅工作在网络层使用路由协议负责包转发的，而是相当于一个“服务器”，需要端口号，甚至需要在应用层（[DHCP](https://baike.baidu.com/item/DHCP)）。

#### NAT

这就是上文问题中提到的NAT协议，及地址端口转化协议。其实这需要端口号，所以严格来讲，不能说是网络层的一个协议。但是**这个NAT是要解决私有IP无法直接与互联网通信的问题。**



#### 关于分类编址、子网掩码、CIDR

1. 子网的引入并不是IPv4地址不够，所以这也是说私有IP同这个问题不是一回事儿的原因。

2. 子网引入的原因就是根据当前网络结构方便规律编址结点，以快速查询通信。
3. 在此基础上，这个编址的规律性实践下来就是子网掩码，而根据子网掩码划分的不同，分为**现代的CIDR**和刚刚讲的分类编址。所谓网络号就是用来划分子网的。
4. 在CIDR中，分类编址规定的只能8、16、24位作为网络号被废弃，所以可以任意位作为网络号，大大增加了子网的数量。
5. 所谓**网络地址（网络号）就是指的一个子网中IP相同的部分**。而**子网掩码就是相同的部分都写1（二进制），不同的部分写0**。
6. 关于CIDR：比如**128.14.35.7/20**是某CIDR地址块中的一个地址，20指的前20位2进制为网络号。



### 进一步讨论子网问题

IP层，也就是路由器要解决什么问题？就是你给了我一个目的ip地址，我要知道给这个ip地址的数据报要发给谁？从哪个端口发出去？

需要建立一张“对应表”---路由表，也就是来了一个IP，我从哪个端口转发，转发给哪个设备，这样才能打包MAC帧，发出去才有“人”要。



#### 假设我们不分子网：

(1) 随机分配(或者按顺序)ip地址给连接在路由器每个端口的设备。按顺序分配IP，今天端口1来了两个设备，分配X.Y.Z.1和X.Y.Z.2。然后这个设备(假设是个笔记本)下班回家了，路由器需要“回收这个设备”，然后端口2来了设备，分配了X.Y.Z.2。明天端口1又来了一个设备，分配X.Y.Z.3，所以按顺序也最终是随机分配。

(2) 固定分配IP，这有两种情况，①就是路由器把每个设备的MAC地址记住，每次它连上发送DHCP让路由器分配IP地址时，就看看是不是对应的MAC地址，然后分给他固定的IP。但这就要求网络规模很小，且设备几乎不更换时。②就是运营商分配的公有IP地址。



以上两种情况，路由器怎么判断，来了一个目的IP为X.Y.Z.278的应该从哪个端口转发，转发给哪个设备呢？每个IP地址由于随机性或固定IP，必须都在路由表中写出来，一一对应。

IP地址很多很多(因为不仅是它直接相连的，所有IP都应该对应，也就是2^32个表项)，若都像这样，一个路由器恐怕得配个4GB内存放路由表，否则就一定有数据报来了，它明明可以转发，却不知道发给谁。并且，这样大的一个路由表，每次查半天，延迟太大了。所以怎么办呢？



首先，很多路由器是现代社会网络(互联网)结构中的“树枝”，它需要访问的大部分IP，只需要无脑转发给他上一级路由就好了。所以我们规定：路由表中0.0.0.0代表默认路由，放在最后一项，若ip地址啥也不是，就当它是0.0.0.0，转发给上一级路由。

<img src="learn-internet.assets/Screen Shot 2021-10-22 at 01.01.30.png" alt="Screen Shot 2021-10-22 at 01.01.30" style="zoom: 33%;" />

那还有很多路由器，处在整个互联网的“中心”，比如有些公司的路由，可能就负责转发的数据相当巨量。然后我们又想到了好办法，就是分配IP地址的时候，不是随机或者顺序分配，而是按照当前的网络结构分配，比如我规定A路由器的LAN1端口，只能是X.Y.0.0-X.Y.127.255号，LAN2端口的X.Y.128.0-X.Y.255.255，这样所有经过A路由器的LAN1端口的IP地址，只需要看是不是X.Y.0-X.Y.127，后15位(为何是15就不展开了)不需要看。这样大大缩减了路由表的大小。



#### 子网掩码

我们需要“告诉”路由器，它需要对比前多少位的IP地址就可以。这就是子网掩码的由来，对人类的话，很容易，比如所有将192.168.0.1~192.168.0.127都划分在端口LAN1，则可以写成192.168.0.0/17，意思是只对比前17位，目的IP地址剩下的几位不管是啥，都无脑走LAN1端口。

但是机器只会做加减、与或非。所以我们可以把前17位写1，后15位写0，路由器拿着它(子网掩码)与目的IP一运算，就得到了前17位相同，后15位都是0的一个地址------我们称其为“网络号”，也就是下图中的“目的网络地址”。

<img src="learn-internet.assets/Screen Shot 2021-10-22 at 11.35.08.png" alt="Screen Shot 2021-10-22 at 11.35.08" style="zoom:50%;" />

从上图路由表很容易可以看出：

**所谓不划分子网，就相当于划分了很多个子网(每个IP地址都是一个“子网”)，子网个数就是需要对照是否一致的IP地址个数。**





不能把 【私有IP/公有IP】与 【子网划分】与【NAT】混淆。





#### IP数据报

![IMG_1851](learn-internet.assets/IMG_1851.jpeg)



#### 路由协议

了解路由，需要先了解现代网络结构，上文「链路层---多路访问」部分较为详细的说明了现代网络结构，我们可以了解到发送一个数据包，需要克服哪些问题。由于网络结构可以大致划分为两层，一个是中心化（树状）的“底”层，与去中心化的核心转发层（骨干网），网络结构的不同，也面临着不同的问题。



##### 关于转发和路由

转发是指的确定了下一跳地址的具体链路接口之间的数据报转发过程，其时间尺度很短；而路由选择是指确定源到目的的最优路径计算过程，其时间尺度很长；转发表是根据路由表确定的下一跳IP地址。

> 摘抄自一篇博客：https://blog.51cto.com/jettcai/1835200
>
> **IP数据包经由路由转发的时候源IP，目的ip是否改变？**
>
> 这是个搞网络的基础问题，答案是不能改变的*，*除非做了nat转换才能改变。否则，数据包在整个传输过程中，源IP和目的IP不会发生改变。
>
> 不过MAC地址是变化的，因为发送端开始不知道目的主机的MAC地址，所以每经过一个路由器，MAC地址都会发生变化。
>
> **目的MAC地址是如何得到的？**
>
> TCP/IP里面是用的ARP协议。比如新建了一个内网，如果一台机器A找机器B，封装Fram时（OSI的第二层用的数据格式），要封装对方的 MAC，开始时A不知道B的MAC，只知道IP，它就发一个ARP包，源IP是自己的，目的IP是B的，源MAC是自己的，目的MAC是广播的。然后这个ARP请求包在内网内被广播，当其他机器接到这个包时，用目的IP和自己的IP比较，不是的话就丢弃。是的话，B接到时，发现IP与自己的一样，就答应这个包的请求，把自己的MAC送给A。如果B是其他子网的机器，那么路由器会判断出B是其他子网，然后路由器把自己的MAC返回给A，A以后再给B发包时，目的MAC封装 的是路由器的。
>
> **路由转发过程：**
>
> 当主机A发向主机B的数据流在网络层封装成IP数据包，IP数据包的首部包含了源地址和目标地址。**主机A会用本机配置的24位IP网络掩码255.255.255.0与目标地址进行与运算，得出目标网络地址与本机的网络地址是不是在同一个网段中。**如果不是将IP数据包转发到网关。
>
> 在发往网关前主机A还会通过ARP的请求获得默认网关的MAC地址。在主机A数据链路层IP数据包封装成以太网数据帧，然后才发住到网关……也就是路由器上的一个端口。
>
> 当网关路由器接收到以太网数据帧时，发现数据帧中的目标MAC地址是自己的某一个端口的物理地址，这时路由器会把以太网数据帧的封装去掉。路由器认为这个IP数据包是要通过自己进行转发，接着它就在匹配路由表。匹配到路由项后，它就将包发往下一条地址。
>
> **路由器转发数据包就是这样，所以它始终是不会改IP地址的。只会改MAC。**
>
> 当有数据包传到路由器时，路由器首先将其的目的地址与路由表进行对比，如果是本地网络，将不会进行转发到外网络，而是直接转发给本地网内的目的主机；**但是如果目的地址经路由表对比，发现不是在本网中，有nat就将改变源地址的IP（原源地址的Ip地址改为了路由器的IP地址），**路由器将数据包转发到相应的端口，进行通信。
>
> 举个例子，如：A访问B： 
> 首先对比是否同一子网，如果是,检查ARP表,有B的MAC就直接发送,没有就发送ARP请求.如果否，发送到默认网关C，源IP为A，源MAC为A，目的IP为B，目的MAC地址为C， 
> C接收到这个包，检查路由表，发送到下一跳D，源IP为A，源MAC为C，目的IP为B，目的MAC为D….. 
> 如此循环,直到发送到B.
>
> **NAT为特殊应用，会修改源IP为网关自己外网IP。**

##### 关于路由

* 要注意路由选择的复杂算法主要是针对网状结构，也就是数据结构中的图，而对于我们家庭网络都是树状结构，比如电脑一般都只连接路由器，那下一跳只会是路由器（所以个人电脑没有路由算法，不需要路由表），家庭路由器也一样，下一跳一定是下一个（可能是小区局域网的路由器）路由器。

* 并不是只有路由器才有路由表，也可以这样理解：有路由表的设备必然运行路由算法程序（对于每路由器控制方式来说），而有路由算法程序的设备大概率是路由器（这个路由器指的我们一般意义上的路由器，就是一台计算机只负责路由、分发网络数据保等网络传输功能）。



路由协议也是建立在IP层，有诸多协议包括[ICMP](https://baike.baidu.com/item/ICMP)、[RIP](https://baike.baidu.com/item/RIP)，[IGRP](https://baike.baidu.com/item/IGRP)（[Cisco](https://baike.baidu.com/item/Cisco)私有协议），[EIGRP](https://baike.baidu.com/item/EIGRP)（Cisco私有协议），[OSPF](https://baike.baidu.com/item/OSPF)，[IS-IS](https://baike.baidu.com/item/IS-IS)，[BGP](https://baike.baidu.com/item/BGP)。是用来“找路”的。使用traceroute就可以查看跳转的路由表(routing table)。

> 路由协议通过在[路由器](https://baike.baidu.com/item/路由器)之间共享路由信息来支持[可路由协议](https://baike.baidu.com/item/可路由协议)。路由信息在相邻[路由器](https://baike.baidu.com/item/路由器)之间传递，确保所有路由器知道到其它路由器的路径。总之，路由协议创建了[路由表](https://baike.baidu.com/item/路由表)，描述了[网络拓扑结构](https://baike.baidu.com/item/网络拓扑结构)；路由协议与[路由器](https://baike.baidu.com/item/路由器)[协同工作](https://baike.baidu.com/item/协同工作)，执行[路由选择](https://baike.baidu.com/item/路由选择)和数据包转发功能。

---
路由选择主要可分为两种方式，首先是有一个大型服务器作为控制器，负责计算路由选择，分发给各个路由器，这叫逻辑集中式控制，现大量应用于大型网络结构。另一种为每路由器控制，就是平级管理，每个路由器都有路由选择的算法，现常用于中小型网络结构。





#### 每路由器控制



##### RIP（距离向量算法）

注意RIP只看跳数。自己直连的网络是可以检测并不被影响的。

<img src="learn-internet.assets/IMG_2099.jpeg" alt="IMG_2099" style="zoom:25%;" />



> #### RIP是抄作业，OSPF是自己做作业。
>
> 小明有天没写数学作业，第二天要交，就抄了同桌小红的。
>
> 抄作业当然简单啊，不用动脑筋，抄就完了，也不用管对错。但是如果小红的答案是错的，那么小明自然也是错的，所以rip属于一种谣传路由，他只是记录下旁边路由器发来的现成的路由表，这些路由条目不用自己计算，机制非常简单，但也因此容易学到错误路由。
>
> 小明自己写作业，自己计算，前提是小明要会计算，这样虽然有点花力气，但能保证结果正确，所以ospf是一种可信度高的路由。ospf是收集网络中的各种信息，然后自己独立构建出拓扑图，再自己独立计算去往每个目标的最佳路由。
>
> 
>
> #### RIP只看跳数，不看实际速度。
>
> 小明有天在教室上课，教室不让带手机，但想给暗恋的对象小红传个纸条。
>
> 这不是同桌小红，是另一个小红，教室对角那个。
>
> RIP的话，在传纸条的时候，只看跳数，也就是小明将采用，间隔同学数量最少的那条路线。
>
> 但是他没考虑到，这条路线有个学霸，学霸一心听课，眼中没有纸条，所以纸条到他那里的时候，停止了传动。一直等到老师休息一下喝茶的时候，学霸才发现来了个纸条，然后继续往后传。
>
> 这条路线虽然间隔同学少，但因为中间有个学霸的缘故，其实反而传的很慢。
>
> 
>
> OSPF的话是这样的，小明先计算每条路径的真实速度，发现另一条路线虽然间隔同学有点多，但这是从后排绕了一下送往小红的，后排是娱乐区，大家很容易被飞来的纸条吸引注意力然后立即再转发出去，所以实现了最快的速度让小纸条到达小红。
>
> 所以，RIP比较路由优劣的方法是，比较到达目标经过路由设备的数量，OSPF比较路由优劣的方法是，比较到达目标的总cost，这个cost值由网络带宽决定，带宽越大越优先。
>
> 
>
> #### RIP可能出现环路，OSPF天生防环。
>
> 路由环路就是，当小明给小红传个纸条，传来传去不知怎么回事又传回来了，或者在中间两个同学来回传，就是一直没到达小红。
>
> rip因为谣传性质，每个节点都有可能学习到错误路由，所以有各种环路的可能。虽然在一次次更新中，增加了一些防环措施，但因为算法的先天缺陷，一直不能彻底杜绝环路。
>
> 
>
> 但是ospf，先天算法防环，**以下内容可能需要网络技术基础。**
>
> 1、本区域内的路由，会使用“迪杰斯特拉”算法进行路由计算，这是个树形的算法，树形天然无环。
>
> 2、区域间的路由，虽然没使用树形的迪杰斯特拉算法，但是采用了星型拓扑来避免环路，也就是中间有个骨干区域0，其他区域不允许直接通信，只能通过区域0来通信，这样就避免了区域间的环路。
>
> 3、汇总后的路由，ospf会自动生成黑洞路由，以防因汇总后，明细路由意外失效再加上默认路由导致的环路情况发生。









#### 逻辑集中式控制







##### [ICMP](https://baike.baidu.com/item/ICMP)

路由协议中ICMP协议非常重要，我们经常使用的`ping`、`tractert`都是基于ICMP协议的。

ICMP 协议应用在许多网络管理命令中，下面以 ping 和 tracert 命令为例详细介绍 ICMP 协议的应用。

（1） ping 命令使用 ICMP 回送请求和应答报文

在网络可达性测试中使用的分组网间探测命令 ping 能产生 ICMP 回送请求和应答报文。目的主机收到 ICMP 回送请求报文后立刻回送应答报文，若源主机能收到 ICMP 回送应答报文，则说明到达该主机的网络正常。

（2）路由分析诊断程序 tracert 使用了 ICMP时间超过报文

tracert 命令主要用来显示数据包到达目的主机所经过的路径。通过执行一个 tracert 到对方主机的命令，返回数据包到达目的主机所经历的路径详细信息，并显示每个路径所消耗的时间。





## Transport layer

![传输层](learn-internet.assets/传输层.png)

**为什么很多问题要到传输层解决呢？**在链路层或者IP层不行吗？答案是可以，但很多时候没必要。我在上文中《链路层》------「每层协议的实现与否有所不同！」部分，提到过几个关键的概念，现再引用自己一下：



> 就像是链路层的可靠传输，这些都是网卡来实现的，而网卡是根据不同的应用场景，功能(某些协议的实现与否)是不一样的。比如上文提到无线传输的网卡会增加可靠传输，来避免到TCP之后再重传，而有线网卡一般就没有这个功能。
>
> 同样地，同样是转发，交换机的网卡实现了自学习构建其转发表，并进行转发，而对于我们的路由器、电脑，其网卡就不会又这个功能，路由器的转发是在网络层通过路由算法来构建的。
>
> 所以不代表，每一个设备，都实现每一层的每一个协议。这个是根据设备的定位灵活取舍的，只要整体的功能实现了就行。
>
> **一定注意每层实现的区别在哪：**所谓传输层做的工作，也就是传到目的地才会干的事儿。
>
> 比如：
>
> ①TCP的GBN实现可靠传输，也就是如果中间路由器丢了这个数据报，这个路由器甚至都是不知道的，只有目的设备超时才会发送“你要重发”的消息。
>
> ②有些无线网络链路层传输时也会用GBN来保证数据传输的可靠性，这时这个数据报若是TCP的，那这就是套娃过程。
>
> 所以：
>
> 很多问题(协议)在所谓传输层考虑，关键在于，往往这些问题只需要这个数据报“到头儿”了才考虑的，而不是路由器的每一跳都要考虑，比如建立连接(IP要是建立连接，路由器每一跳都要先连接一遍，延迟太大了)



### UDP

主要是两个问题：一台计算机有很多应用“同时访问”不同的网站，但一个IP只标注了一台计算机，就需要另外的协议把计算机上每个应用进程的请求数据区分开，也就是UDP协议！

用的就是端口(Port)，这也是这层封装的重要概念，而UDP还有很多缺陷，比如没有确认链接稳定等机制，这在有些重要连贯信息的传输上就很麻烦，所以TCP就是解决了这些问题，但是也正是这些机制处理需要时间，TCP协议要比UDP慢，所以像视频通话这种丢失数据不是很影响整体效果且需要大量带宽的应用，还是使用UDP。

### TCP

[TCP-wiki](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)、

> [一篇很不错的博客](): 
> <img src="../../../6.jpg" alt="TCP 头格式" style="zoom: 33%;" />
>
> <img src="https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/12.jpg" alt="UDP 头部格式" style="zoom:50%;" />
>

####  TCP报文格式

<img src="learn-internet.assets/6.jpeg" alt="6" style="zoom:25%;" />

> 引用自《王道书---网络》
>
> **1)源端口和目的端口**。各占2B。端口是运输层与应用层的服务接口,运输层的复用和分用功能都要通过端口实现。
>
> **2)序号**。占4B,范围为0~22-1,共22个序号。TCP是面向字节流的(即TCP传送时是逐个字节传送的),所以TCP连接传送的字节流中的每个字节都按顺序编号。序号字段的值指的是本报文段所发送的数据的第一个字节的序号例如,一报文段的序号字段值是301,而携带的数据共有100B,表明本报文段的数据的最后一个字节的序号是400,因此下一个报文段的数据序号应从401开始。
>
> **3)确认号**。占4B,是期望收到对方下一个报文段的第一个数据字节的序号。若确认号为N, 则表明到序号N-1为止的所有数据都已正确收到。
>
> 例如,B正确收到了A发送过来的一个报文段,其序号字段是501,而数据长度是200B(序号501~700),这表明B正确收到了A发送的到序号700为止的数据。因此B期望收到A 的下一个数据序号是701,于是B在发送给A的确认报文段中把确认号置为701 
>
> 4)数据偏移(即**首部长度**)。占4位,这里不是P数据报分片的那个数据偏移,而是表示首部长度(首部中还有长度不确定的选项字段),它指出TCP报文段的数据起始处距离TCP报文段的起始处有多远。“数据偏移”的单位是32位(以4B为计算单位)。因此当此字段的值为15时,达到TCP首部的最大长度60B 
>
> 5)保留。占6位,保留为今后使用,但目前应置为0。
>
> 6)紧急位URG。URG=1时,表明紧急指针字段有效。它告诉系统此报文段中有紧急数据, 应尽快传送(相当于高优先级的数据)。但URG需要和紧急指针配合使用,即数据从第个字节到紧急指针所指字节就是紧急数据。
>
> **7)确认位ACK**。仅当ACK=1时确认号字段オ有效。当ACK=0时,确认号无效。
>
> TCP规定,在连接建立后所有传送的报文段都必须把ACK置1。
>
> 8)推送位PSH(Push)。接收方TCP收到PSH=1的报文段,就尽快地交付给接收应用进程, 而不再等到整个缓存都填满后再向上交付
>
> 9)复位位RST( Reset)。RST=1时,表明TCP连接中出现严重差错(如主机崩溃或其他原因),必须释放连接,然后再重新建立运输连接
>
> **10)同步位SYN**。同步SYN=1表示这是一个连接请求或连接接受报文当SYN=1,ACK=0时,表明这是一个连接请求报文,对方若同意建立连接,则应在响应报文中使用SYN=1,ACK=1 
>
> **11)终止位FIN**( Finish)。用来释放一个连接。当FIN=1时,表明此报文段的发送方的数据已发送完毕,并要求释放传输连接。
>
> **12)窗ロ**。占2B,范围为0~216-1。它指出现在允许对方发送的数据量,接收方的数据缓存空间是有限的,因此用窗口值作为接收方让发送方设置其发送窗口的依据。
>
> 例如,设确认号是701,窗口字段是1000。这表明,从701号算起,发送此报文段的一方还有接收1000字节数据(字节序号为701~1700)的接收缓存空间。
>
> 13)校验和。占2B。校验和字段检验的范围包括首部和数据两部分。在计算校验和时,和UDP一样,要在TCP报文段的前面加上12B的伪首部(只需将UDP伪首部的第4个字段,即协议字段的17改成6,其他的利UDP一样)。
>
> 14)紧急指针。占2B。紧急指针仅在URG=1时才有意义,它指出在本报文段中紧急数据共有多少字节(紧急数据在报文段数据的最前面)。
>
> 15)选项。长度可变。TCP最初只规定了一种选项,即最大报文段长度( Maximum Segment Size, MSS)。MSS是TCP报文段中的数据字段的最大长度(注意仅仅是数据字段) 16)填充。这是为了使整个首部长度是4B的整数倍



#### [端口号规则](https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xml)

0～1023端口号被口头规定为熟知端口号（Well-known port），这部分端口号默认每个都对应着固定的应用层的某一个通用协议。

> 在[RFC 6335](http://tools.ietf.org/html/rfc6335)中解释这一点：
>
> > 动态端口范围（49152-65535）中的端口已专门留作本地和动态使用，不能通过IANA进行分配。应用程序软件可以简单地使用本地主机上可用的任何动态端口，而无需进行任何分配。另一方面，应用程序软件不得假定“动态端口”范围内的特定端口号始终始终可用于通信，因此该范围内的端口号不得用作服务标识符。
>
> 保留的端口：
>
> > 用户端口范围（1024-49151）中的端口可通过IANA进行分配，并且在成功分配后可用作服务标识符。
> >
> > 也可以通过IANA分配“系统端口”范围（0-1023）中的端口。由于“系统端口”范围既是最小端口，也是分配最密集的端口，所以新分配的要求比“用户端口”范围的要求更为严格，并且仅在“ IETF审查”或“ IESG批准”过程[RFC5226中](http://tools.ietf.org/html/rfc5226)被授予。
>
> 引言解释了这种困惑：
>
> > 多年来，与传输控制协议（TCP）[RFC0793]和用户数据报协议（UDP）[RFC0768]一起使用的新服务名称和端口号值的分配不够明确。
>
> 看来Windows XP没有遵循RFC6335，但是Solaris 10遵循了。



**[端口复用](https://www.zhihu.com/question/342296674)**



**Q：**TCP为什么需要三次握手？

A：因为要建立双向连接，双方都需要相互确认对方信息：

	  1. A发送A的序号（并将SYN标志位设为1等操作）
	  2. B收到后回复（A的序号+1），且同时发送B的序号
	  3. A收到后回复（B的序号+1），并同时发送第一波包裹

值得注意的是，**这个序列号和SYN标志位不仅仅是有些比喻为打招呼那么简单，而是避免了与旧报文混淆。**但这样讲还是说不清楚这个问题，《计算机网络---自顶向下》这本书的该部分内容思路就非常棒，完整的展示了，为什么我们需要这么多参数来保证这个“可靠性”传输。下节重点讲，



#### 面向连接传输、加密传输、可靠性传输

* 可靠性传输与网络安全完全是两码事：

可靠性是担心环境、非人为因素导致数据损坏、丢失；安全问题是防止人为因素导致数据泄露。

* 面向连接与可靠性传输也不是一回事：

面向连接就是在传输数据之前的交换信息。但是不一定意味着可靠传输，后者是指数据的传输都有确认机制。



所以TCP是不安全的(没有加密)、可靠的、面向连接的传输协议。





#### TCP与可靠传输实现！

1. 首先来看若没有任何措施，发送会出现哪些意外情况：

![IMG_1270](learn-internet.assets/IMG_1270.jpeg)

a）也就是收到了数据，但是出错了，甚至都不会意识到自己的数据错误；

b）也就是对丢包，接收方根本没收到，发送方不管，直接继续发。



2. 解决上述两个问题：

**首先是针对a）：**

（1）肯定要加入bit检验位，也就是校验码，这是《组成原理》中学过的，不展开。

（2）但是有很多情况是知道错误但是还原不出原有数据，所以这时候需要一个重传的机制，也就是接收方要跟发送方报告接收到的数据是否完整，所以就有了**ACK/NAK**（分别代表：没问题/有误需重发）。

![IMG_1272](learn-internet.assets/IMG_1272.jpeg)

c）所示，改进后出现bit错误没问题了；但是看d），当ACK/NAK包出错时，没有办法；另外如e）ACK丢包也是不行。

**针对b）、e）丢包问题：**

（3）我们可以让**发送方设置等待时间t**，若到时间未收到ACK/NAK包，则直接重发。

**针对d）的ACK/NAK包错误问题：**

（4）同样可以让发送方遇到错误的ACK/NAK包，NAK包，到时间t未收到包，都重发数据包。

但此时又有了新的问题：就是这个包的序号0、1只有上帝视角知道，然而当接收方发送了ACK后，他会等待下一组数据，然而中途若ACK丢包或者ACK解析错误，发送方会重发，而此时接收方误以为下一组数据。这个问题很好解决，就是给**分组数据编号**。所以经过了（3）、（4）的改进，TCP的基本逻辑已经成型了。

<img src="learn-internet.assets/IMG_1273.jpeg" alt="IMG_1273" style="zoom:33%;" />

上图几种可能出现的错误情况都没问题，但是效率的问题就来了，这样发送太慢了，我们能不能多组一起发？可以，但问题就会复杂起来，这就是所谓的流水线问题：(更像是多发射？)

<img src="learn-internet.assets/Screen Shot 2021-08-14 at 22.03.32.png" alt="Screen Shot 2021-08-14 at 22.03.32" style="zoom:33%;" />

这怎么确认呢？**GBN（Go-Back-N）协议**来了：

<img src="learn-internet.assets/Screen Shot 2021-08-14 at 22.06.30.png" alt="Screen Shot 2021-08-14 at 22.06.30" style="zoom:33%;" />



#### GBN与SR

##### GBN延迟“拖累”窗口发送的情况

作为对比，这种情况与分组存储转发的平均传输速率、总时间等问题，见上文章节《平均传输速率》。

<img src="learn-internet.assets/IMG_1936.jpeg" alt="IMG_1936" style="zoom:25%;" />

延迟是看第一个包的总延迟。

比如这题不用GBN协议，而是分组-存储转发机制。则理论最大的平均传输速率为100Mb/s。这与GBN的区别在于：**GBN协议若窗口会停止滑动(最后一包发送后，第一个包确认帧没有收到)，则每个窗口“周期”，都会有相同的时间等待，则延迟就会变成周期性地，从而影响了传输速率**。



用之前的大队、小队的例子。GBN就是我一个大队出队之后，必须有一个小队回来，才能发送下一个小队，且限制是未确认回来的小队数量是大队长/小队长。

这样，最理想的情况是，若我大队在完全出队之前，第一个小队确认回来，就可以持续出队，理论最大传输速率不会受延迟影响。

而若我发送完大队后的xms后才收到，则速度就会变成**原传输速度*[大队发送完时间/(大队发送完时间+xms)]**。因为**等待是每个大队(窗口)周期性地发生的**。因为**等待是每个大队(窗口)周期性地发生的**。



So: 

**① 信道利用率=max{1, 窗口发送时间/第一个数据报来回总时间}**

**② 平均最大传输速率=理论最大传输速率\*信道利用率。**

**③ 第一个数据报来回总时间Tf = 数据报发送时间T1 + 确认帧发送时间T1 + 来回传播时间T3 + 中间节点存储转发传输时间T4。**

其中：

* T1 = 一个数据报大小/极限传输速率。
* T2 = 一个确认帧大小/极限传输速率。
* T4需要表明确认帧与数据帧一样大，才能直接乘2。

若作出一点假设「注意前提条件」，我们可以得到：在GBN协议下，信道利用率与几个变量的关系，是很简单的正比或反比关系，下面是推导：

<img src="learn-internet.assets/IMG_2243.jpeg" alt="IMG_2243" style="zoom: 25%;" />

而且，想象这样一种情况：**语音电话使用GBN协议**。（所谓语音电话就是将麦克风的信号实时编码并压缩打包后发送，假设编码压缩打包的速度正好等于链路传输速度）

这是若是题目中的条件，则会出现延迟越来越大，我和对面回话“反应”越来越慢的现象。这就相当于，排队等待出的队伍速度还是100Mb/s，但是出站速度成了80Mb/s，所以站内(网络包缓冲区？)越来越挤了。**这就需要拥塞/流量控制**的算法来解决了。

但是，语音不会用TCP的(但应该也不会是无脑转发的UDP)，所以它实时编码的“码率”如果超过了平均传输速度，则就还是会出现上述这种情况，比如我们看视频时选择固定的1080p视频，则就经常加载，如果把这个视频理解成对面实时的“话”，那么这延迟明显越来越大。

正如流量/拥塞控制的思想：我们可以根据发送及收到确认帧的情况，来调整发送的“码率”，这也是看视频时「自动清晰度选项」背后工作的核心原理。



##### 发送序号位数n与窗口大小w的关系

GBN与SR是遵循一样的规则：**发送窗口数Ws+接收窗口数Wr≤2^n，才没有歧义**。



因为接受窗口收到后发送确认帧并移动窗口，此时若发送的新帧序号无法与「发送端为收到确认帧后超时重发的旧帧」区分，则就会出错。

极限情况就是接收窗口移动了Ws，而发送窗口没有移动(也就是Ws个确认帧都已发送但都丢失)。此时新的接受窗口Ws号到Ws+Wr号需要与0-Ws编号不同，也就是共需要Ws+Wr个不同的编号。



所以，GBN中发送窗口最大是2^n -1，因为接收窗口是1；SR中发送窗口最大是2^(n-1)，因为接收窗口与发送窗口等大才能都利用上。



下图是例题：

> 注意：假设是GBN，接收窗口为1，有3位作为发送序号，则窗口数为7，不代表序号是0-6，而序号还是0-7循环发。(这样才能区分，像上文讲的那种问题)

<img src="learn-internet.assets/IMG_2059.jpeg" alt="IMG_2059" style="zoom: 33%;" />





#### 一些问题

**Q：IP数据就已经分包了，TCP是在此基础上按应用请求分包？还是和IP是融合的？TCP给分包编号，那IP那一层编号吗？**

A：在实体数据层面（光纤或网线的光电信号），一个包是既有TCP表头也有IP表头的，但是他们编号是不同的，TCP是以端口（）



**Q：这层的核心应该在于数据传输实现了进程到进程的抽象（端口）。为什么在这层建立可靠传输，而不是网络层或者应用层？**

A：**网络传输中这些工作（路由选择、地址编码、可靠传输、安全性等等）是必须的，其他的都不是必须的**，尤其是所谓的分层问题，这更多的是历史遗留问题，我们可不可以只分两层：设备层、进程层？大概率也是没什么问题的，就是把这些实现方式重新打包一下，协议该“浓缩”成一个就浓缩一下，理想化的网络协议结构会精简的多。

- 首先，当然可以。但是在那个时代，哪有这么多进程啥的，IP、路由协议，主要还是考虑设备到设备的通信，所以最大的原因是没有考虑到进程这一层面的通信问题，然后就有人在此基础上发明了新的协议来解决这个问题，就像是一层层的补丁（先是UDP用端口号来“独立”进程间的通信，然后TCP就“补”上了验证的机制，让通信可靠一点）。

- 要说为什么不在网络层实现，还是有一点点原因的？因为很多延迟敏感、速度敏感，却对丢包相对 不敏感的网络应用很多，如视频直播、网络游戏等。它们倾向于“不可靠”传输。很明显，这种选择（到底是可靠速度慢还是不可靠速度快）是在进程这一层开始思考的问题，而对于设备（网络层）这一层来说，“无脑”转发，做 以一个设备的身份（所有进程）进行数据传输 要做的那部分工作最好。

当然，HTTP这一层也可以在UDP的基础上实现“自己”的可靠传输，事实上也有这样的做法了———**QUIC**（Quick UDP Internet Connection）。

> https://www.chromium.org/quic
>
> QUIC is a new multiplexed transport built on top of UDP. HTTP/3 is designed to take advantage of QUIC's features, including lack of Head-Of-Line blocking between streams.
>
> The QUIC project started as an alternative to TCP+TLS+HTTP/2, with the goal of improving user experience, particularly page load times. The [QUIC working group](https://datatracker.ietf.org/wg/quic/about/) at the IETF defined a clear boundary between the transport([QUIC](https://datatracker.ietf.org/doc/html/rfc9000)) and application(HTTP/3) layers, as well as migrating from QUIC Crypto to [TLS 1.3](https://datatracker.ietf.org/doc/html/rfc8446).
>
> Because TCP is implemented in operating system kernels and middleboxes, widely deploying significant changes to TCP is next to impossible. However, since QUIC is built on top of UDP and the transport functionality is encrypted, it suffers from no such limitations.
>
> **Key features of QUIC and HTTP/3 over TCP+TLS and HTTP/2 include** 
>
> - Reduced connection establishment time - 0 round trips in the common case
> - Improved congestion control feedback
> - Multiplexing without head of line blocking
> - Connection migration
> - Transport extensibility
> - Optional unreliable delivery



### TCP拥塞控制与流量控制的全过程



#### 流量控制和拥塞控制什么区别与联系？

①原因不一样，所以触发条件不一样；

②本质方式一样，都是在滑动窗口的基础上，让发送方减小发送窗口。

③具体方式不同，流量控制由于信息直接来自接受方，所以可以很“精确”的“告知”发送方你最快发多快；而拥塞控制几乎只能考大致响应时间(比如超时)或接下来讲到的3次ACK，来预估可能网络不好，发生了拥塞，然后降低窗口大小，然而正因这种不确定性，让这个策略比流量控制复杂。

> 拥塞控制是让网络能够承受现有的网络负荷,是一个全局性的过程,涉及所有的主机、所有的路由器,以及与降低网络传输性能有关的所有因素。相反,流控制往往是指点对点的通信量的控制,是个端到端的问题(接收端控制发送端),它所要做的是抑制发送端发送数据的速率,以便使接收端来得及接收。当然,拥塞控制和流量控制也有相似的地方,即它们都通过控制发送方发送数据的速率来达到控制效果。



#### 流量控制

若是接收方接受的数据超过了自己缓冲区处理速度的极限，就会更改TCP报文中的窗口字段，减少“接收窗口”大小。 （接收都是给对方看的，所以TCP报文的窗口都表示接受窗口大小，而发送窗口只需要本地“知道”就可以了）



#### 快重传

**快重传本质就是：人家后面的数据都到了，前面还有没到的，大概率丢了，不用等了，重发！**

冗余ACK就是再次确认某个报文段的ACK,而发送方先前已经收到过该报文段的确认。所谓冗余ACK，就是没到超时重传的时候，但后面的先到，就发“冗余”ACK(期望收到前面还没到的，虽然它还没超时)。规定3个同样的ACK，就重发，即使没超时。



#### 拥塞控制

虽然有时候接收设备的上限远远没有达到，但是由于中间设备(路由等)处理速度等限制了发送的速度，甚至会丢包(比如可能是IP层超过了最大路由跳数)。这种情况称为拥塞。所以能感受到与流量区别。

具体几种算法，会应用在在传输过程不同的阶段。(慢开始、拥塞避免、快恢复)



> 下图来自《王道书---网络》

<img src="learn-internet.assets/IMG_2244.jpeg" alt="IMG_2244" style="zoom: 25%;" />

拥塞之后再次慢开始得策略基本已被弃用(也不完全是，因为3次ACK可能没超时，但是超时可能还是会发生)，换用下面的快恢复。

<img src="learn-internet.assets/IMG_2245.jpeg" alt="IMG_2245" style="zoom:25%;" />

 要结合快重传的冗余ACK。（ACK=1，表示确认字段有效，具体见TCP报文结构）

①这种情况，往往是网络状况可能拥塞，但不一定，所以没必要“反应过激”，就是拥塞窗口减半。

②要注意及时网络不会拥塞，这个拥塞避免（加法增大），发送窗口基本会遇到接收窗口(流量控制，也就是接收设备的接收速度上限)的上限。



### TCP异常处理

> 原问题：TCP连接中，一端断电和一端进程崩溃时，表现有何区别？
>
> 作者：小林coding
> 链接：https://zhuanlan.zhihu.com/p/390939380
> 来源：知乎
> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
>
> 
>
> 这个属于 **TCP 异常断开连接**的场景，这部分内容在我的「图解网络」还没有详细介绍过，这次就乘着这次机会补一补。
>
> 
>
> <img src="https://pic3.zhimg.com/v2-4fe8462c745135a66283a3d3f1bc45de_b.jpg" alt="img" style="zoom:25%;" />
>
> 
>
> 这个问题有几个关键词：
>
> - 没有开启 keepalive；
> - 一直没有数据交互；
> - 进程崩溃；
> - 主机崩溃；
>
> 我们先来认识认识什么是 TCP keepalive 呢？
>
> 这东西其实就是 **TCP 的保活机制**，它的工作原理我之前的文章写过，这里就直接贴下以前的内容。
>
> 
>
> <img src="https://pic4.zhimg.com/v2-c3fcc2b7b1db546b4a45b1a4ad7e87df_b.jpg" alt="img" style="zoom:25%;" />
>
> 
>
> 如果两端的 TCP 连接一直没有数据交互，达到了触发 TCP 保活机制的条件，那么内核里的 TCP 协议栈就会发送探测报文。
>
> - 如果**对端程序是正常工作**的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。
> - 如果**对端主机崩溃，或对端由于其他原因导致报文不可达**。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。
>
> 所以，TCP 保活机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活。
>
> 
>
> <img src="https://pic4.zhimg.com/v2-e87e86b6a5bbccc348d059b82a170837_b.jpg" alt="img" style="zoom:25%;" />
>
> 
>
> 注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。
>
> 知道了 TCP keepalive 作用，我们再回过头看题目中的「主机崩溃」这种情况。
>
> > 在没有开启 TCP keepalive，且双方一直没有数据交互的情况下，如果客户端的「主机崩溃」了，会发生什么。
>
> 如果客户端主机崩溃了，服务端是**无法感知到的**，在加上服务端没有开启 TCP keepalive，又没有数据交互的情况下，**服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态**，直到服务端重启进程。
>
> 所以，我们可以得知一个点。
>
> 在没有使用 TCP 保活机制，且双方不传输数据的情况下，一方的 TCP 连接处在 ESTABLISHED 状态时，并不代表另一方的 TCP 连接还一定是正常的。
>
> > 那题目中的「进程崩溃」的情况呢？
>
> 我自己做了个实验，使用 kill -9 来模拟进程崩溃的情况，发现**在 kill 掉进程后，服务端会发送 FIN 报文，与客户端进行四次挥手**。
>
> 所以，即使没有开启 TCP keepalive，且双方也没有数据交互的情况下，如果其中一方的进程发生了崩溃，这个过程操作系统是可以感知的到的，于是就会发送 FIN 报文给对方，然后与对方进行 TCP 四次挥手。
>
> 以上就是对这个面试题的回答。
>
> 这面试题其实在变相考察 TCP 保活机制的作用。
>
> 
>
> <img src="https://pic1.zhimg.com/v2-ac16be259ed58b6b7a6010a59ba8ff24_b.jpg" alt="img" style="zoom:25%;" />
>
> 接下来我们看看在「**有数据传输**」的场景下的一些异常情况：
>
> - 第一种，客户端主机宕机，又迅速重启，会发生什么？
> - 第二种，客户端主机宕机，一直没有重启，会发生什么？
>
> ### **客户端主机宕机，又迅速重启**
>
> 在客户端主机宕机后，服务端向客户端发送的报文会得不到任何的响应，在一定时长后，服务端就会触发**超时重传**机制，重传未得到响应的报文。
>
> 服务端重传报文的过程中，刚好客户端主机重启完成，这时客户端的内核就会接收重传的报文，：
>
> - 如果客户端主机上**没有**进程监听该 TCP 报文的目标端口号，由于找不到目标端口，客户端内核就会**回复 RST 报文，重置该 TCP 连接**；
> - 如果客户端主机上**有**进程监听该 TCP 报文的目标端口号，由于客户端主机重启后，之前的 TCP 连接的数据结构已经丢失了，客户端内核里协议栈会发现找不到该 TCP 连接的 socket 结构体，于是就会**回复 RST 报文，重置该 TCP 连接。**
>
> 所以，只要有一方重启完成后，收到之前 TCP 连接的报文，都会回复 RST 报文，以断开连接。
>
> ### **客户端主机宕机，一直没有重启**
>
> 这种情况，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了。
>
> 那具体重传几次呢？
>
> 在 Linux 系统中，提供了一个叫 tcp_retries2 配置项，默认值是 15，如下图：
>
> 
>
> ![img](https://pic1.zhimg.com/v2-e9ee17fa6da444ac66d41303b62e658c_b.png)
>
> 
>
> 这个内核参数是控制，在 TCP 连接建立的情况下，超时重传的最大次数。
>
> 不过 tcp_retries2 设置了 15 次，并不代表 TCP 超时重传了 15 次才会通知应用程序终止该 TCP 连接，内核还会基于「最大超时时间」来判定。
>
> 每一轮的超时时间都是**倍数增长**的，比如第一次触发超时重传是在 2s 后，第二次则是在 4s 后，第三次则是 8s 后，以此类推。
>
> 
>
> <img src="https://pic1.zhimg.com/v2-7d02f0b3a8907f2747348c88ab1d8040_b.jpg" alt="img" style="zoom:25%;" />
>
> 
>
> 内核会根据 tcp_retries2 设置的值，计算出一个最大超时时间。
>
> 
>
> <img src="https://pic2.zhimg.com/v2-3efb7e9fa159b7d98b8bf303d56530a5_b.jpg" alt="img" style="zoom:25%;" />
>
> 
>
> **在重传报文且一直没有收到对方响应的情况时，先达到「最大重传次数」或者「最大超时时间」这两个的其中一个条件后，就会停止重传**。
>
> 最后说句。
>
> TCP 牛逼，啥异常都考虑到了







## Application layer

<img src="learn-internet.assets/应用层.png" alt="应用层" style="zoom: 25%;" />

有了MAC和IP，我们就可以在整个互联网中不同的子网络跳转，相互访问；再有了Port，计算机也实现了多进程并发请求。这些就构成了当今计算机网络世界的骨干。但这还不够，就像是有了10和加法器，逻辑运算器，理论上就可以编写一切程序，但是后来还是有函数的定义，再到后来的面向对象，这些对实现功能可能没有太大帮助，但是这种人性化的设计会大大减少人学习和Debug的时间，而这也是不可忽视的，这些就属于应用层。

### 回顾HTTP前的数据报

![数据报](learn-internet.assets/数据报.jpeg)



### SSH/Telnet

这与“常见”的HTTP不同，是应用层的另一个分支，用于远程终端。

### FTP

顾名思义，文件传输协议。在讲述这个协议的具体规范的之前，我们应当思考它为什么存在？可以扩展为：**文件传输与信息传输有何本质区别，以至于需要一系列单独的协议来实现？**









### DNS

我们用域名()来一一对应的描述IP地址，这就像是IP相较于MAC地址，让层级关系更明确，我们更容易记忆和检索，但是计算机服务器还是需要IP，千万乃至上亿网络服务器的域名和IP的对应关系就存在DNS服务器上，当你键入www.google.com时候，浏览器首先会去DNS服务器问其对应的IP地址，返回来之后再进行我们之前说的那些网络传输流程。

而且，为了检索方便，DNS的域名是以树这种数据结构存储的，比如www.google.com就是一个二级域名，而www.mail.google.com就是一个在www.google.com底下的三级域名。



#### 域名解析过程







##### 递归与迭代结合：

![Screen Shot 2021-07-04 at 11.28.51 AM](learn-internet.assets/Screen Shot 2021-07-04 at 11.28.51 AM.png)

这种方式缓解了根DNS、TLD服务器的压力，也提高了综合查询效率（相比递归查询）

1. 迭代就是本地域名服务器自己找：

   它先找根服务器，根服务器返回给本地域名服务器 顶级域名服务器的地址，本地域名服务器自己去找顶级域名服务器。

2. 之所以是结合，那是因为主机与本地域名服务器是递归查询，否则就应该是主机自己去访问根域名服务器了

3. 说明本地域名服务器与其他域名服务器（根、顶级）与主机都不是采用同一种算法。在这种查询方式下：

   根DNS、TLD仅作为DNS服务端；

   本地域名服务器即作为服务端又作为客户端；

   主机仅作为客户端。

### HTTP

以上均为连接到目标服务器，但是连接上服务器之后，具体请求某个特定页面，我们还没有标准，这就是[HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview)的作用了。



<div align = "center" >
    <img src="learn-internet.assets/HTTP_layers.png" style="zoom:33%;" />
</div>



当我们在浏览器键入www.google.com和www.mail.google.com时，DNS服务器会帮我们返回对应的IP，但是我们可能访问这个服务器的部分内容，比如常见的：www.apple.com/cn 在2019年更新成了www.apple.com.cn这个看似微小的改变实际上是苹果中国官网单独使用了一个服务器。而www.apple.com和www.apple.com/cn是同一个域名（也就是www.apple.com），同样得到同一个IP，也就是在同一个服务器上。这个/cn就是在向这个服务器请求cn页面/文件，所用的协议/标准就是HTTP，而这个‘www.apple.com/cn’被称作**[URL](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL)**。



[**Q: http中get和post指令区别？**](https://foofish.net/understand-http.html)

A：HTTP 请求由3部分组成，分别是请求行、请求首部、请求体，首部和请求体是可选的，并不是每个请求都需要的。

![http-request.jpg](learn-internet.assets/http-request.jpg)

**请求行**

请求行是每个请求必不可少的部分，它由3部分组成，分别是请求方法（method)、请求URL（URI）、HTTP协议版本，以空格隔开。

HTTP协议中最常用的请求方法有：GET、POST、PUT、DELETE。GET 方法用于从服务器获取资源，90%的爬虫都是基于GET请求抓取数据。

请求 URL 是指资源所在服务器的路径地址，比如上图的例子表示客户端想获取 index.html 这个资源，它的路径在服务器 foofish.net 的根目录（/）下面。

**请求首部**

因为请求行所携带的信息量非常有限，以至于客户端还有很多想向服务器要说的事情不得不放在请求首部（Header），请求首部用于给服务器提供一些额外的信息，比如 `User-Agent` 用来表明客户端的身份，让服务器知道你是来自浏览器的请求还是爬虫，是来自 Chrome 浏览器还是 FireFox。HTTP/1.1 规定了47种首部字段类型。HTTP首部字段的格式很像 Python 中的字典类型，由键值对组成，中间用冒号隔开。比如：

```
User-Agent: Mozilla/5.0
```

因为客户端发送请求时，发送的数据（报文）是由字符串构成的，为了区分请求首部的结尾和请求体的开始，用一个空行来表示，遇到空行时，就表示这是首部的结尾，请求体的开始。

### 请求体

请求体是客户端提交给服务器的真正内容，比如用户登录时的需要用的用户名和密码，比如文件上传的数据，比如注册用户信息时提交的表单信息。

现在我们用 Python 提供的最原始API *socket* 模块来模拟向服务器发起一个 HTTP 请求

```python
with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
    # 1. 与服务器建立连接
    s.connect(("www.seriot.ch", 80))
    # 2. 构建请求行，请求资源是 index.php
    request_line = b"GET /index.php HTTP/1.1"
    # 3. 构建请求首部，指定主机名
    headers = b"Host: seriot.ch"
    # 4. 用空行标记请求首部的结束位置
    blank_line = b"\r\n"

    # 请求行、首部、空行这3部分内容用换行符分隔，组成一个请求报文字符串
    # 发送给服务器
    message = b"\r\n".join([request_line, headers, blank_line])
    s.send(message)

    # 服务器返回的响应内容稍后进行分析
    response = s.recv(1024)
    print(response)
```

---

接下来是另一部分：

### https 三次握手(非对称加密)

1，客户端输入https网址，链接到server443端口；

2，服务器手中有一把钥匙和一个锁头，把锁头传递给客户端。数字证书既是公钥，又是锁头

3，客户端拿到锁头后，生成一个随机数，用锁头把随机数锁起来（加密），再传递给服务器。这个随机数成为私钥，现在只有客户端知道

4，服务器用钥匙打开锁头，得到随机数。该随机数变成了新的锁头，把内容锁起来（加密），再传递给客户端。这个随机数服务器也知道了，并且用它来加密数据

5，客户端用自己手中的钥匙（随机数），解密后得到内容。客户端用私钥解密数据

6，接下来的客户端和服务器的数据交换都通过这个随机数来加密。只有客户端和服务器知道私钥，所以数据传输是安全的，不是明文的



---
## System API（并不属于上述的每一层）



操作系统配合浏览器会帮你实现上述所有功能，甚至更多那。我不用浏览器怎么自己创建链接呢？自己看完所有协议按照协议写表头吗？太不现实了，操作系统帮我们打包好了协议，然后给了我们端口，IP等自由度（其实就是函数或者python所谓的方法），叫**Socket（这要和socks代理区分开）**。

* 而且有一个非常容易误解的地方：就是**关于Socket接口和TCP/IP协议的关系问题**：有些讲socket的会讲到TCP等协议，就会让人想到：那我在这个网络传输协议整个框架中怎么没有看到过呢？这就是操作系统的“视角”，和功能实现的“视角”。**现在操作系统把内存管理，文件系统，图形界面，网络传输的诸多协议与标准都分类打包好，做成了API，而Socket就是操作系统关于网络传输那一部分打包好的API，自然包含了这些协议。**

举个例子，我们也会想到图形界面是怎么实现的：问题太多，比如字体的每个像素点的相对分布，然后色彩管理，窗口的移动，缩放等等。但是我们编写一个软件，当然会有图形界面，但我们不需要去了解这方面的细节，只需懂一些原理就好，然后利用操作系统提供的图形API，做好内容，窗口之类的不用管太多。这跟网络这一方面是一样的。

* [socket-python博客](https://keelii.gitbooks.io/socket-programming-in-python-cn/content/)

### 代理

* pac文件规则：
```bash
=== 通配符支持 => *
*.example.com/ 代表 http://example.com http://233.example.com https://233.example.com https://666.example.com/233.mp4 全部走代理。
同时"*"可省略，.example.com/ 与 *.example.com/ 效果是一样的
 
=== 正则表达式支持
以 \ 开始和结束，\[\w]+:\/\/example.com\
 
=== 例外规则 => @@
@@*.example.com/ 表示"@@"后面的网址规则(*.example.com)不走代理
如：@@www.baidu.com 表示 www.baidu.com 不走代理
 
=== 匹配地址开始和结尾规则 => |
|http://example.com、example.com| 分别表示 以http://example.com开始 和 以example.com结束 的地址
如：|http://233.com ，代表 http://233.com 开头的网址才会走代理，即 https://233.com http://1.233.com 都不会走代理
如：233.com|，代表 233.com 结尾的网站才会走代理，即 http://233.com https://233.com http://1.233.com 都会走带了，而 http://233.com/index.html 不会走代理。
 
=== 全匹配规则 => ||
||example.com 则代表 http://example.com、https://example.com、ftp://example.com 等协议的地址全部走代理
如：||233.com ，即 http://233.com、https://233.com、ftp://233.com 等地址全都走代理
 
=== 注释规则 => !
!我是注释233
!我也是注释666
```



## Web 服务器(服务端)

> 引用自：https://wangzitian0.github.io/2017/06/27/WEB-server-explore/
>
> ### 几种不同名字的服务器有什么区别？
>
> WEB服务器、应用程序服务器、HTTP服务器
>
> 具体的内容可以看 [这篇文章](http://www.javaworld.com/article/2077354/learn-java/app-server-web-server-what-s-the-difference.html) 和它的 [翻译版](http://blog.csdn.net/flykobesummer/article/details/5024304)
>
> 我只说结论：
>
> - HTTP服务器本质上也是一种应用程序
> - Web服务器的基本功能就是提供Web信息浏览服务。它只需支持HTTP协议、HTML文档格式及URL。与客户端的网络浏览器配合。因为Web服务器主要支持的协议就是HTTP，所以通常情况下HTTP服务器和WEB服务器是相等的，说的是一回事。
> - 应用服务器可能包含PC机上运行的GUI进程，web服务器，甚至其他的app服务器。app服务器和客户端之间的通信并不局限于简单的显示标记，而是可以由程序逻辑，比如数据表单、方法调用，而非静态的HTML。这样，客户端程序就可以按需去用。
> - 举个例子，QQ 的后台是应用程序服务器，QQ 空间的后台是WEB服务器。
>
> ### tomcat 与 nginx，apache的区别是什么？
>
> 知乎有一个关于这个问题的答案 [tomcat 与 nginx，apache的区别是什么？](https://www.zhihu.com/question/32212996)
> 还有一个 Nginx 和 Apache 的对比 [三大Web服务器对比分析](http://www.sohu.com/a/128940634_468650)
>
> 总的来说：
>
> - 应用程序服务器
>   - Tomcat能够动态的生成资源并返回到客户端。
>     - Java Servlet技术以及衍生的Java Server Pages技术可以让Java程序也具有处理HTTP请求并且返回内容。
>     - 它有直接部署 java 程序的能力
>   - Tomcat运行在JVM之上，它和HTTP服务器一样，绑定IP地址并监听TCP端口
>     - 管理Servlet程序的生命周期
>     - 将URL映射到指定的Servlet进行处理
>     - 与Servlet程序合作处理HTTP请求
> - web 服务器
>   - Apache和Nginx都能够将某一个静态资源文件的内容通过HTTP协议返回到客户端。
>     - 无论何时、任何人访问它得到的内容都是完全相同的
>   - Apache和Nginx可以通过其他模块来支持动态资源
>     - 通过Shell、PHP、Python脚本程序来动态生成内容
>     - 其他语言可以通过 CGI、WSGI 等协议接入 web 服务器。
>   - Apache vs Nginx
>     - Apache 的 rewrite 更强大，社区模块非常多，bug极少
>     - nginx 轻量，用 epoll可以扛并发，可以反向代理
>
> ### 各种语言的简易web服务器
>
> 这个地方的 web 服务器应该理解为只能访问静态资源的 web 服务器。
>
> 换句话说，在硬盘某个地方放一些文件，搭起服务器后，可以在浏览器里通过网络协议来访问这些资源。
>
> [C++](https://github.com/eidheim/Simple-Web-Server)
>
> [Golang](https://github.com/itang/gohttp)
>
> [Python](http://coolshell.cn/articles/1480.html)
>
> [Nodejs](https://stackoverflow.com/questions/6084360/using-node-js-as-a-simple-web-server)
>
> - 推荐 light-server
>
> [PHP](https://github.com/youngj/httpserver)
>
> ### 正经网站的结构
>
> 这部分内容的结构来源于【[大型网站技术架构](https://book.douban.com/subject/25723064/)】，然后加上我自己的理解和常见的工具、现代化 MVP 处理方式。
>
> 正经网站基本上都需要需要存储结构化数据，有不同身份用户交互的，需要存储音乐图片视频等媒体文件的，然后是用网络协议调用外部服务。
>
> - 在书里说是 xxx 服务器，但更准确的说法是 xxx 服务，比如缓存，我们很多时候可以和应用程序使用同一个服务器硬件。
> - 更现代的方式应该是监控各种服务的资源占用情况，然后根据资源占用情况混合部署，充分利用 CPU、内存、硬盘 IO 性能等等
> - 这里可以把 xxx 服务器理解为抽象概念的服务器，哪怕调用 memcache 这样的插件也是走网络协议 socket 或者 http 协议
> - 各个部件用不同的协议和端口，所以抽象层逻辑上的单机和多机区别不大，集群带来的效果更多关注总的 CPU 算力、总内存、总硬盘 IO 量等等。





## 浏览器内核(客户端)

[chromium知乎](https://www.zhihu.com/question/290767285/answer/1200063036)

虽然从操作系统底层就有基本网络协议的API，但是现代网络数据解码收发的主要角色还是浏览器内核，我们可以从Webkit说起，当然不可避免要谈Chrome内核，上述的链接文章就总结的不错。![IMG_0411](learn-internet.assets/IMG_0411.JPG)


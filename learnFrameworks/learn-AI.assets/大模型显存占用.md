# 大模型显存占用与选型指南

**结论**：Gemma-2-27B 这种体量的模型，即使做了 QAT/4bit 量化，在 24GB 显存的 4090 上也基本只能"舒适地"服务 1 个用户、短上下文。想要支持 10–20 个并发用户，同时维持高上下文（几千 token）和流畅速度，实际需要 48GB 甚至 80GB 级别显存，或者多卡部署。

---

## 1. 4090 上的显存分布分析

### 1.1 物理限制

- RTX 4090 物理显存为 **24GB GDDR6X**，这是硬上限，不可能在单卡上真正使用超过 24GB 显存。
- 如果观察到 30–35GB 的占用数字，可能是以下原因：
  - 看的是"显存 + 共享内存 / 统一内存 / 进程总占用"的数字（如 Windows 任务管理器）；
  - 机器里有多张卡，或者有多个进程在占显存，总和为 30–35GB；
  - 启用了 CUDA 统一内存 / offload，数据被换到主存甚至 NVMe，上层监控把它一并计入。

**重要**：一旦 24GB 被占满，框架就会开始频繁换页 / 溢出到 CPU 或 NVMe，推理速度会断崖式下降。

### 1.2 LLM 推理时的显存构成

以 Gemma-2-27B-IT 为例，模型参数规模：

- 参数量：27B
- hidden size：4096，层数 46，context 长度官方支持到 8192 tokens

推理时显存大致可拆分为以下几部分：

#### 1.2.1 模型权重（Weights）

- **决定因素**：参数量 × 精度（FP16 / BF16 / 8bit / 4bit 等）
- **实测经验**：Gemma / Gemma3 27B 做 4bit QAT/GGUF 时，权重本体大概在 **16–18GB** 左右，可以刚好塞进 24GB 卡，还留一点空间。
- HuggingFace 社区用户反馈：单轮对话大概占用 18GB 显存，上下文拉长后在 4090 上存在超过 24GB 的风险。

#### 1.2.2 KV Cache（注意力键值缓存）

- **决定因素**：
  - 上下文长度（prompt + 已生成的 token 数）
  - 模型结构（层数、hidden size、是否 GQA）
  - KV 的精度（通常是 FP16/BF16，除非专门做了 KV 量化）

- **理论估算**（帮助建立直觉）：
  对 Gemma-2-27B（46 层、hidden ≈ 4096），若 KV 用 2 字节（BF16/FP16）存储：
  - 8K context 单流的 KV cache 量级大约在 **5–6GB** 左右
  - 4K context 则大约 **2.5–3GB**

  （实际与框架实现会有 20–30% 的偏差，但量级是对的。）

- **实测数据**：Gemma3 27B Q4 在 4090 上，模型加载后权重占用 ≈17GB；context 拉到 20K tokens 时总显存约 21GB 左右，几乎吃满 24GB。

#### 1.2.3 中间激活 / 临时张量

- **决定因素**：batch size、sequence length、实现细节（FlashAttention 等）
- 现代推理框架（vLLM、TGI 等）会把这块压到比较小，一般是几百 MB 到 1–2GB 的量级。

#### 1.2.4 框架和 CUDA 预留

- PyTorch / CUDA 会预留一部分显存做 allocator pool，在 nvidia-smi 里通常能看到 1–2GB 的"隐性占用"。

#### 1.2.5 多进程 / 多模型副本

- 如果 Python 后端是 gunicorn / uvicorn 多 worker，每个 worker 自己 `from_pretrained` 一次，那么每个进程都有一份 17–18GB 的权重拷贝，显存会立即爆掉。
- 很多"单用户、多次调用"的场景，实际上每次调用都走了不同的进程 / 新建了 pipeline / 没有复用同一个模型对象，导致显存持续上涨。

### 1.3 "单用户多次调用"导致显存爆满的常见原因

#### 1.3.1 重复加载或复制模型

- 例如在每个请求 handler 里调用 `AutoModel.from_pretrained(...)`，或者在不同线程 / 进程里各自实例化模型。
- 结果：以为是"一个用户多次调用"，实际是"多个模型副本"在抢 24GB。

#### 1.3.2 每次调用都重新拼 prompt，不复用 KV cache

- **正常做法**：一次对话在框架内部维护 KV cache，后续 token 只在 cache 上增量计算。
- **错误做法**：在 Python 层反复把整段历史拼成一个大 prompt，再从头 generate，导致：
  - 有效上下文长度持续变长；
  - KV cache 等价于"每轮都重新算一遍"，激活和临时张量膨胀，算力被浪费。

#### 1.3.3 显存溢出触发 Unified Memory / Offload

- 超过 24GB 后，PyTorch / bitsandbytes / 一些推理框架会自动把 KV cache 或部分激活挪到 CPU / NVMe 上。
- 此时每生成一个 token 都要跨 PCIe 来回搬数据，吞吐量和首 token 延迟会瞬间变得非常难看——这就是"推理速度断崖下降"的原因。

---

## 2. 10–20 个用户的显卡选型

以下分析基于公开数据，会标注数据来源，明确区分"测出来的"和"理论估算"。

### 2.1 Gemma-27B 基准数据

几条与 27B 模型相关的已知数据：

1. **HuggingFace 社区用户实测**：gemma-2-27b-it 单轮对话大约用 18GB 显存，长上下文可以把 4090 24GB 用满。
2. **Titan RTX（24GB）实测**：加载 Gemma3 27B 的 4bit 量化版，权重占用约 17GB 显存，剩下 7GB 给 KV 和其他开销。
3. **4090 实测**：Gemma3 27B Q4KM，context=20K 时总显存约 21GB，只剩 ~1GB 余量。

**小结**：在 24GB 卡上，27B 模型 + 中长上下文时，权重 ≈17–18GB，KV + 激活轻松把剩下的 6–7GB 吃光。

### 2.2 并发场景的显存增长规律

Basebox 硬件 sizing 案例（13B 模型、8K context）：

| 并发用户数 | 显存占用 |
|-----------|---------|
| 1         | ~15GB   |
| 4         | ~18GB   |
| 8         | ~22GB   |
| 16        | ~30GB   |

**规律**：

- 权重是主力（一次性固定开销）；
- 并发数主要增加 KV cache 部分，增长是次线性的（因为并发用户通常不会都撑满最大 context，且框架有优化）。

### 2.3 Gemma-2-27B 并发估算

结合以上数据和 Gemma-2-27B 的结构参数（46 层、hidden 4096），在 **4bit 权重 + BF16 KV、上下文 4K 左右**的场景下：

| 项目 | 显存占用 |
|-----|---------|
| 权重 | 17–18GB |
| 单用户 KV + 激活 | ~1.5–2GB |
| 10 个活跃用户 | 额外 15–20GB |
| **总计** | **32–38GB** |

### 2.4 单卡 4090 24GB 的并发能力

综合实测 + 理论估算：

- **Gemma-2-27B 4bit + 4090 24GB**：
  - 1–2 个用户，context 在 2K 左右：比较舒适
  - 4–6 个用户并发：勉强可行，但必须严格控制每个会话的最大 context（如 1–2K），并且 KV 要做量化或 offload
  - 10–20 个用户、上下文几千 token：单卡 4090 几乎只能靠重度 offload（CPU / NVMe / 网络 KV cache），会明显牺牲延迟与吞吐，体验上基本达不到"顺滑"的程度

**结论**："显存爆了 → 速度断崖"不是偶然，而是 24GB 在 **27B + 长上下文 + 多次调用**这个组合下的必然结果。

### 2.5 显卡选型建议

以下分三档说明，针对开发 / 小团队场景，不考虑上万卡的大集群。

#### 档位 A：48GB 级别工作站 / 数据中心卡（推荐起步）

**代表型号**：

- **RTX 6000 Ada**：48GB GDDR6 ECC，Ada 架构，内存带宽约 960 GB/s，面向工作站和 AI 推理
- **NVIDIA L40S**：48GB GDDR6 ECC、Ada 架构，面向机房部署

**为什么 48GB 是合理下限**：

- Gemma-2-27B 4bit 权重 ~18GB
- 10 个用户，每人 4K 左右上下文，KV + 激活估计 15–20GB
- 加上框架预留和安全裕度，总共在 35–40GB 上下
- 48GB 卡还有 8–10GB 的缓冲空间，可以：
  - 支撑偶发的"非常长的某一轮对话"
  - 适当提升 batch size（提高吞吐）
  - 避免频繁触发 offload

48GB 级别（RTX 6000 Ada / L40S）被普遍认为是"适合 20–30B 模型、多用户推理"的主流档位。

**现实预期**：

- 48GB 单卡 + vLLM / TGI 这类现代推理框架，在合理限制上下文（如 4K 默认上限）的情况下：
  - 10 个并发用户：整体体验比较舒服
  - 20 个并发用户：仍可运行，但需要更激进的 KV 量化和调度（动态批处理、分级优先队列），延迟会比 10 并发情况高一些

#### 档位 B：80GB HBM 数据中心卡（预算充足时的稳妥方案）

**代表型号**：

- **NVIDIA A100 80GB HBM2e**：80GB 高带宽显存，带宽约 2TB/s，支持 MIG 切片
- **NVIDIA H100 80GB / 94GB HBM3**：80–94GB HBM3，显存带宽 3.3–3.9TB/s，推理吞吐大约是 A100 的 1.5–2 倍，是 2025 年 AI 训练 / 推理的标杆卡

**80GB 的优势**：

1. Gemma-2-27B 权重只占 18GB 左右，有 60GB 空间给 KV + 批量并发：
   - 可以把 per-user 最大上下文设得比较激进（8K 甚至 16K），同时支撑 20 个以上并发
   - 即便用户"打一大段长文"，也不容易触发 offload

2. HBM 带宽 + 更强算力，保证在高并发下仍有不错的 tokens/s：
   - 4090 算力不弱，但 GDDR6X 带宽与数据中心卡的 HBM3 还是有明显差距
   - 在 20 并发、每个用户都在生成长回复时，高带宽显存在保持吞吐和延迟上优势明显

如果预算允许且部署环境适合机房级别 GPU，1× A100 80GB 或 1× H100 80GB 对于"Gemma-27B + 10–20 用户"的场景基本是"碾压级别"的安全选择。

#### 档位 C：多块 24GB 消费卡（多 4090 / 50 系）

如果倾向于沿用 4090 这类消费卡，可以考虑：

- 用 2–4 张 24GB 卡，每张卡上只跑一个模型副本，后端层面做负载均衡。
- 例如 2× 4090：
  - 每张卡跑一个 Gemma-27B 4bit 模型实例
  - 每个实例负责大约 5–8 个并发用户
  - 总并发可以做到 10–16，体验上类似一张 48GB，但需要在应用层实现调度与容错

**Trade-off**：

| 优点 | 缺点 |
|-----|-----|
| 单卡成本相对低，适合从"已有一张 4090"往上扩 | 权重会在每张卡上各自复制一份（2×18GB、3×18GB…），不能像大显存卡那样把多并发全压在一份权重上 |
| 对软件栈的要求比较直观：就是"多 worker + 负载均衡" | 机箱、电源、散热压力都更大 |
| | 不像 A100/H100 有 NVLink/MIG 这类更成熟的多租户支持 |

### 2.6 软件栈优化建议

即使换到了 48GB 或 80GB 卡，要想高并发下体验稳定，还建议做以下优化：

#### 2.6.1 统一模型服务层

- 不在业务后端里直接 `from_pretrained`
- 推荐用 vLLM / TGI / Triton / Ollama 之类作为"单一模型服务"进程
- 后端通过 HTTP/gRPC 调用，这样 GPU 侧只有一个进程管理显存，容易控制并发和批处理

#### 2.6.2 KV cache 友好的框架和配置

- 启用 paged KV / FlashAttention
- 视情况把 KV cache 做成 8bit 量化（很多框架已有 KV quant 支持），可以有效降低并发带来的显存放大

#### 2.6.3 限制单会话最大上下文

- 例如把历史对话裁剪成"最近 N 轮 + 摘要"
- 避免个别用户把 context 拉到 16K 导致所有人体验变差

#### 2.6.4 监控与容量规划

- 结合 nvidia-smi、Prometheus 等监控工具观测：
  - 并发数 vs 显存占用
  - tokens/s、首 token 延迟
- 和公开测量 / 计算器做对标，逐步验证自己的容量模型

---

## 3. 总结

在"Gemma-2-27B-QAT + 10–20 个用户正常体验"的前提下：

1. **24GB 的 4090** 更适合作为单用户 / 小并发开发卡，不适合直接支撑 10–20 个重度长对话用户。
2. **要稳妥支持 10 个左右的并发用户**，建议至少上一张 48GB 级别的 RTX 6000 Ada / L40S。
3. **要兼顾 20 并发 + 更长上下文**，80GB 级别（A100/H100 或新一代 80–96GB 工作站卡）会更安心。
4. 在此基础上，再通过 **vLLM/TGI + KV 量化 + 合理的上下文管理**，才能真正做到"多用户体验稳定"。
